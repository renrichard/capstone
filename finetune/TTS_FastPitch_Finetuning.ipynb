{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WP4yUnh-xu1k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WP4yUnh-xu1k",
    "outputId": "b6d2eb6a-a449-454a-8f22-e5e5d85bc287",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jasonjjl1999/data’: File exists\n",
      "fatal: destination path 'VCC2020-database' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/data\n",
    "!git -C ~/data clone https://github.com/nii-yamagishilab/VCC2020-database.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/data/VCC2020-database/extract\n",
    "!mkdir ~/data/VCC2020-database/extract\n",
    "\n",
    "!unzip ~/data/VCC2020-database/vcc2020_database_training_target_task1.zip -d ~/data/VCC2020-database/extract/\n",
    "!unzip ~/data/VCC2020-database/vcc2020_database_transcriptions.zip -d ~/data/VCC2020-database/extract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf capstone\n",
    "# !git clone -b tts-finetune https://github.com/renrichard/capstone.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cea817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_lightning\n",
    "# !apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
    "# !pip install Cython\n",
    "# !pip install nemo_toolkit['all']\n",
    "\n",
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# %cd apex\n",
    "# !pip install -v --disable-pip-version-check --no-cache-dir ./\n",
    "# %cd ..\n",
    "\n",
    "# !wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/nemo/tts_en_e2e_fastpitchhifigan/versions/1.0.0/zip -O tts_en_e2e_fastpitchhifigan_1.0.0.zip -P ~/checkpoints\n",
    "\n",
    "# !unzip ~/content/tts_en_e2e_fastpitchhifigan_1.0.0.zip -d ~/checkpoint\n",
    "\n",
    "# !gdown https://drive.google.com/uc?id=15FoehxQEZN8OSoIg8am7Wq-7vLS7QIgu\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/tts/conf/fastpitch_align.yaml\n",
    "\n",
    "# !git clone https://github.com/kaldi-asr/kaldi.git kaldi --origin upstream\n",
    "\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449872f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('capstone')\n",
    "\n",
    "# from capstone.preprocess.json.create_vcc_2020_json import create_vcc_2020_json\n",
    "# create_vcc_2020_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bbac2",
   "metadata": {
    "id": "8d0bbac2"
   },
   "source": [
    "# Finetuning FastPitch for a new speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d063299",
   "metadata": {
    "id": "2d063299"
   },
   "source": [
    "In this tutorial, we will finetune a single speaker FastPitch (with alignment) model on limited amount of new speaker's data. We cover two finetuning techniques: \n",
    "1. We finetune the model parameters only on new speaker's text and speech pairs; \n",
    "2. We add a learnable speaker embedding layer to the model, and finetune on a mixture of original speaker's and new speaker's data.\n",
    "\n",
    "We will first prepare filelists containing the audiopaths and text of the samples on which we wish to finetune the model, then generate and run a training command to finetune Fastpitch on 5 mins of data, and finally synthesize the audio from the trained checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502cf61",
   "metadata": {
    "id": "2502cf61"
   },
   "source": [
    "## Creating filelists for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1563d",
   "metadata": {
    "id": "b7b1563d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from capstone.preprocess.json.vcc_2020_paths import data_dir, filelist_dir, exp_base_dir\n",
    "\n",
    "def make_sub_file_list(speaker_id, num_samples, total_duration_mins, seed=42):\n",
    "\t\"\"\"\n",
    "\tCreates a subset of training data for a HiFiTTS speaker. Specify either the num_samples or total_duration_mins\n",
    "\tSaves the filelist in the filelist_dir. split is either \"train\" or \"dev\"\n",
    "\n",
    "\tArguments:\n",
    "\tspeaker_id -- speaker id of the new HiFiTTS speaker\n",
    "\tclean_other -- \"clean\" or \"other\" depending on type of data of new HiFiTTS speaker\n",
    "\tsplit -- \"train\" or \"dev\"\n",
    "\tnum_samples -- Number samples of new speaker (set None if specifying total_duration_mins)\n",
    "\ttotal_duration_mins -- Total duration of new speaker's data (set None if specifying num_samples)\n",
    "\t\"\"\"\n",
    "\tfile_list_name = \"{}_metadata.json\".format(speaker_id)\n",
    "\twith open(os.path.join(data_dir, file_list_name), 'r') as f:\n",
    "\t\tall_records = [json.loads(l) for l in f.read().split(\"\\n\") if len(l) > 0]\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(all_records)\n",
    "\n",
    "\tif num_samples is not None and total_duration_mins is None:\n",
    "\t\tsub_records = all_records[:num_samples]\n",
    "\t\tfname_extension = \"ns_{}\".format(num_samples)\n",
    "\telif num_samples is None and total_duration_mins is not None:\n",
    "\t\tsub_record_duration = 0.0\n",
    "\t\tsub_records = []\n",
    "\t\tfor r in all_records:\n",
    "\t\t\tsub_record_duration += r['duration']\n",
    "\t\t\tif sub_record_duration > total_duration_mins * 60.0:\n",
    "\t\t\t\tprint(\"Duration reached {} mins using {} records\".format(total_duration_mins, len(sub_records)))\n",
    "\t\t\t\tbreak\n",
    "\t\t\tsub_records.append(r)\n",
    "\t\tfname_extension = \"dur_{}_mins\".format(int(round(total_duration_mins)))\n",
    "\telif num_samples is None and total_duration_mins is None:\n",
    "\t\tsub_records = all_records\n",
    "\t\tfname_extension = \"ns_all\"\n",
    "\telse:\n",
    "\t\traise NotImplementedError()\n",
    "\tprint(\"num sub records\", len(sub_records))\n",
    "\n",
    "\tif not os.path.exists(filelist_dir):\n",
    "\t\tos.makedirs(filelist_dir)\n",
    "\n",
    "\ttarget_fp = os.path.join(filelist_dir, \"{}_metadata_{}_local.json\".format(speaker_id, fname_extension))\n",
    "\twith open(target_fp, 'w') as f:\n",
    "\t\tfor record in json.loads(json.dumps(sub_records)):\n",
    "\t\t\trecord['audio_filepath'] = os.path.join(data_dir, record['audio_filepath'])\n",
    "\t\t\tf.write(json.dumps(record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c635928",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c635928",
    "outputId": "62a29d7a-3703-46eb-8e6c-654c3d0b5a64"
   },
   "outputs": [],
   "source": [
    "make_sub_file_list('TEF1', None, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75d1d5",
   "metadata": {
    "id": "ef75d1d5"
   },
   "source": [
    "## Finetuning the model on filelists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fcfec",
   "metadata": {
    "id": "a57fcfec"
   },
   "outputs": [],
   "source": [
    "# pitch statistics of the new speakers\n",
    "# These can be computed from the pitch contours extracted using librosa yin\n",
    "# Finetuning can still work without these, but we get better results using speaker specific pitch stats\n",
    "# pitch_stats = {\n",
    "#     92 : {\n",
    "#         'mean' : 214.5, # female speaker\n",
    "#         'std' : 30.9,\n",
    "#         'fmin' : 80,\n",
    "#         'fmax' : 512\n",
    "#     },\n",
    "#     6097 : {\n",
    "#         'mean' : 121.9, # male speaker\n",
    "#         'std' : 23.1,\n",
    "#         'fmin' : 30,\n",
    "#         'fmax' : 512\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "def generate_training_command(new_speaker_id, duration_mins, mixing_enabled, original_speaker_id, ckpt, use_new_pitch_stats=False):\n",
    "    \"\"\"\n",
    "    Generates the training command string to be run from the NeMo/ directory. Assumes we have created the finetuning filelists\n",
    "    using the instructions given above.\n",
    "    \n",
    "    Arguments:\n",
    "    new_speaker_id -- speaker id of the new HiFiTTS speaker\n",
    "    duration_mins -- total minutes of the new speaker data (same as that used for creating the filelists)\n",
    "    mixing_enabled -- True or False depending on whether we want to mix the original speaker data or not\n",
    "    original_speaker_id -- speaker id of the original HiFiTTS speaker\n",
    "    use_new_pitch_stats -- whether to use pitch_stats dictionary given above or not\n",
    "    ckpt: Path to pretrained FastPitch checkpoint\n",
    "    \n",
    "    Returns:\n",
    "    Training command string\n",
    "    \"\"\"\n",
    "    def _find_epochs(duration_mins, mixing_enabled, n_orig=None):\n",
    "        # estimated num of epochs \n",
    "        if duration_mins == 5:\n",
    "            epochs = 1000\n",
    "        elif duration_mins == 30:\n",
    "            epochs = 300\n",
    "        elif duration_mins == 60:\n",
    "            epochs = 150\n",
    "        \n",
    "        if mixing_enabled:\n",
    "            if duration_mins == 5:\n",
    "                epochs = epochs/50 + 1\n",
    "            elif duration_mins == 30:\n",
    "                epochs = epochs/10 + 1\n",
    "            elif duration_mins == 60:\n",
    "                epochs = epochs/5 + 1\n",
    "        \n",
    "        return int(epochs)\n",
    "            \n",
    "            \n",
    "    if ckpt.endswith(\".nemo\"):\n",
    "        ckpt_arg_name = \"init_from_nemo_model\"\n",
    "    else:\n",
    "        ckpt_arg_name = \"init_from_ptl_ckpt\"\n",
    "    if not mixing_enabled:\n",
    "        train_dataset = \"{}_metadata_dur_{}_mins_local.json\".format(new_speaker_id, duration_mins)\n",
    "        val_dataset = \"{}_mainifest_dev_ns_all_local.json\".format(new_speaker_id)\n",
    "        prior_folder = os.path.join(data_dir, \"Priors{}\".format(new_speaker_id))\n",
    "        exp_dir = \"{}_to_{}_no_mixing_{}_mins\".format(original_speaker_id, new_speaker_id, duration_mins)\n",
    "        n_speakers = 1\n",
    "    else:\n",
    "        train_dataset = \"{}_mainifest_train_dur_{}_mins_local_mix_{}.json\".format(new_speaker_id, duration_mins, original_speaker_id)\n",
    "        val_dataset = \"{}_mainifest_dev_ns_all_local.json\".format(new_speaker_id)\n",
    "        prior_folder = os.path.join(data_dir, \"Priors_{}_mix_{}\".format(new_speaker_id, original_speaker_id))\n",
    "        exp_dir = \"{}_to_{}_mixing_{}_mins\".format(original_speaker_id, new_speaker_id, duration_mins)\n",
    "        n_speakers = 2\n",
    "    train_dataset = os.path.join(filelist_dir, train_dataset)\n",
    "    val_dataset = os.path.join(filelist_dir, val_dataset)\n",
    "    exp_dir = os.path.join(exp_base_dir, exp_dir)\n",
    "                                    \n",
    "    max_epochs = _find_epochs(duration_mins, mixing_enabled, n_orig=None)\n",
    "    config_name = \"fastpitch_align_44100.yaml\"\n",
    "    \n",
    "    training_command = \"python /content/capstone/finetune/fastpitch2_finetune.py --config-name={} train_dataset={} validation_datasets={} +{}={} trainer.max_epochs={} trainer.check_val_every_n_epoch=1 prior_folder={} model.train_ds.dataloader_params.batch_size=24 model.validation_ds.dataloader_params.batch_size=24 exp_manager.exp_dir={} model.n_speakers={}\".format(\n",
    "        config_name, train_dataset, val_dataset, ckpt_arg_name, ckpt, max_epochs, prior_folder, exp_dir, n_speakers)\n",
    "    # if use_new_pitch_stats:\n",
    "    #     training_command += \" model.pitch_avg={} model.pitch_std={} model.pitch_fmin={} model.pitch_fmax={}\".format(\n",
    "    #         pitch_stats[new_speaker_id]['mean'], \n",
    "    #         pitch_stats[new_speaker_id]['std'],\n",
    "    #         pitch_stats[new_speaker_id]['fmin'],\n",
    "    #         pitch_stats[new_speaker_id]['fmax']\n",
    "    #     )\n",
    "    training_command += \" model.optim.lr=2e-4 ~model.optim.sched\"\n",
    "    return training_command\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c55af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f98c55af",
    "outputId": "a9de45b1-b157-45bc-95ad-f45ce8008f75"
   },
   "outputs": [],
   "source": [
    "new_speaker_id = \"TEF1\"\n",
    "duration_mins = 5\n",
    "mixing = False\n",
    "original_speaker_id = \"TEM1\"\n",
    "ckpt_path = \"/root/checkpoint\"\n",
    "print(generate_training_command(new_speaker_id, duration_mins, mixing, original_speaker_id, ckpt_path, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.is_available()\n",
    "# torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r7qzu3RnHpE1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7qzu3RnHpE1",
    "outputId": "0cfce6d6-34a3-4bd9-a5ed-40d4c41a977e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ~/capstone/capstone/finetune/fastpitch2_finetune.py --config-name=fastpitch_align.yaml train_dataset=/home/ryan/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json validation_datasets=/home/ryan/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json +init_from_nemo_model=/home/ryan/capstone/FastPitch-Align-LJSpeech.nemo trainer.max_epochs=5000 trainer.check_val_every_n_epoch=1 prior_folder=/home/ryan/data/VCC2020-database/extract/target_task1/json/PriorsTEF1 model.train_ds.dataloader_params.batch_size=24 model.validation_ds.dataloader_params.batch_size=24 exp_manager.exp_dir=/home/ryan/data/VCC2020-database/extract/target_task1/exp_base/TEM1_to_TEF1_no_mixing_5_mins model.n_speakers=1 model.optim.lr=2e-4 ~model.optim.sched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdf1ed",
   "metadata": {
    "id": "c3bdf1ed"
   },
   "source": [
    "## Synthesize samples from finetuned checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b46325",
   "metadata": {
    "id": "f2b46325"
   },
   "source": [
    "Once we have finetuned our FastPitch model, we can synthesize the audio samples for given text using the following inference steps. We use a HiFiGAN vocoder trained on multiple speakers, get the trained checkpoint path for our trained model and synthesize audio for a given text as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c91dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "886c91dc",
    "outputId": "a3e0836c-2419-4f90-de74-be000a69d3de"
   },
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "hifigan_ckpt_path =  \"/root/checkpoint\"\n",
    "# vocoder = HifiGanModel.load_from_checkpoint(hifigan_ckpt_path)\n",
    "vocoder = HifiGanModel.from_pretrained(\"tts_hifigan\")\n",
    "vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8802b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel \n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "\n",
    "fp = FastPitchModel.restore_from(\"./FastPitch-Align-LJSpeech_5k.nemo\")  # The file shared above\n",
    "hf = HifiGanModel.from_pretrained(\"tts_hifigan\")  # This will fetch our publicily available model from the cloud\n",
    "\n",
    "tokens = fp.parse(\"Hello world! I am a generated speaker.\").cuda()\n",
    "with torch.no_grad():\n",
    "    spectrogram = fp.generate_spectrogram(tokens=tokens)\n",
    "    audio = hf(spec=spectrogram).squeeze(1)\n",
    "ipd.display(ipd.Audio(audio.cpu().numpy(), rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c986f",
   "metadata": {
    "id": "0a4c986f"
   },
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker = None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Arguments:\n",
    "    spec_gen_model -- Instance of FastPitch model\n",
    "    vocoder_model -- Instance of a vocoder model (HiFiGAN in our case)\n",
    "    str_input -- Text input for the synthesis\n",
    "    speaker -- Speaker number (in the case of a multi-speaker model -- in the mixing case)\n",
    "    \n",
    "    Returns:\n",
    "    spectrogram, waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    parser_model = spec_gen_model\n",
    "    with torch.no_grad():\n",
    "        parsed = parser_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().cuda()\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker = speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio\n",
    "\n",
    "def get_best_ckpt(experiment_base_dir, new_speaker_id, duration_mins, mixing_enabled, original_speaker_id):\n",
    "    \"\"\"\n",
    "    Gives the model checkpoint paths of an experiment  we ran. \n",
    "    \n",
    "    Arguments:\n",
    "    experiment_base_dir -- Base experiment directory (specified on top of this notebook as exp_base_dir)\n",
    "    new_speaker_id -- Speaker id of new HiFiTTS speaker we finetuned FastPitch on\n",
    "    duration_mins -- total minutes of the new speaker data\n",
    "    mixing_enabled -- True or False depending on whether we want to mix the original speaker data or not\n",
    "    original_speaker_id -- speaker id of the original HiFiTTS speaker\n",
    "    \n",
    "    Returns:\n",
    "    List of all checkpoint paths sorted by validation error, Last checkpoint path\n",
    "    \"\"\"\n",
    "    if not mixing_enabled:\n",
    "        exp_dir = \"{}/{}_to_{}_no_mixing_{}_mins\".format(experiment_base_dir, original_speaker_id, new_speaker_id, duration_mins)\n",
    "    else:\n",
    "        exp_dir = \"{}/{}_to_{}_mixing_{}_mins\".format(experiment_base_dir, original_speaker_id, new_speaker_id, duration_mins)\n",
    "    \n",
    "    ckpt_candidates = []\n",
    "    last_ckpt = None\n",
    "    for root, dirs, files in os.walk(exp_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".ckpt\"):\n",
    "                val_error = float(file.split(\"v_loss=\")[1].split(\"-epoch\")[0])\n",
    "                if \"last\" in file:\n",
    "                    last_ckpt = os.path.join(root, file)\n",
    "                ckpt_candidates.append( (val_error, os.path.join(root, file)))\n",
    "    ckpt_candidates.sort()\n",
    "    \n",
    "    return ckpt_candidates, last_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153bd5a",
   "metadata": {
    "id": "0153bd5a"
   },
   "source": [
    "Specify the speaker id, duration mins and mixing variable to find the relevant checkpoint from the exp_base_dir and compare the synthesized audio with validation samples of the new speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901f88b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "8901f88b",
    "outputId": "1b4ba935-cf31-47d5-88d2-117e90bc3120"
   },
   "outputs": [],
   "source": [
    "new_speaker_id = \"TEF1\"\n",
    "duration_mins = 5\n",
    "mixing = False\n",
    "original_speaker_id = \"TEM1\"\n",
    "\n",
    "\n",
    "_ ,last_ckpt = get_best_ckpt(exp_base_dir, new_speaker_id, duration_mins, mixing, original_speaker_id)\n",
    "print(last_ckpt)\n",
    "\n",
    "cfg = {'name': 'FastPitch', 'sample_rate': 44100, 'train_dataset': '/root/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json', 'validation_datasets': '/root/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json', 'prior_folder': '/root/data/VCC2020-database/extract/target_task1/json/PriorsTEF1', 'model': {'learn_alignment': True, 'n_speakers': 1, 'symbols_embedding_dim': 384, 'max_token_duration': 75, 'n_mel_channels': 80, 'pitch_embedding_kernel_size': 3, 'n_window_size': 2048, 'n_window_stride': 512, 'fmax': None, 'pitch_fmin': 80, 'pitch_fmax': 640, 'pitch_avg': 211.27540199742586, 'pitch_std': 52.1851002822779, 'train_ds': {'dataset': {'_target_': 'nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset', 'manifest_filepath': '${train_dataset}', 'max_duration': None, 'min_duration': 0.1, 'int_values': False, 'normalize': True, 'sample_rate': '${sample_rate}', 'trim': False, 'sup_data_path': '${prior_folder}', 'n_window_stride': '${model.n_window_stride}', 'n_window_size': '${model.n_window_size}', 'pitch_fmin': '${model.pitch_fmin}', 'pitch_fmax': '${model.pitch_fmax}', 'pitch_avg': '${model.pitch_avg}', 'pitch_std': '${model.pitch_std}', 'vocab': {'notation': 'phonemes', 'punct': True, 'spaces': True, 'stresses': True, 'add_blank_at': 'None', 'pad_with_space': True, 'chars': True, 'improved_version_g2p': True}}, 'dataloader_params': {'drop_last': False, 'shuffle': True, 'batch_size': 24, 'num_workers': 12}}, 'validation_ds': {'dataset': {'_target_': 'nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset', 'manifest_filepath': '${validation_datasets}', 'max_duration': None, 'min_duration': None, 'int_values': False, 'normalize': True, 'sample_rate': '${sample_rate}', 'trim': False, 'sup_data_path': '${prior_folder}', 'n_window_stride': '${model.n_window_stride}', 'n_window_size': '${model.n_window_size}', 'pitch_fmin': '${model.pitch_fmin}', 'pitch_fmax': '${model.pitch_fmax}', 'pitch_avg': '${model.pitch_avg}', 'pitch_std': '${model.pitch_std}', 'vocab': {'notation': 'phonemes', 'punct': True, 'spaces': True, 'stresses': True, 'add_blank_at': 'None', 'pad_with_space': True, 'chars': True, 'improved_version_g2p': True}}, 'dataloader_params': {'drop_last': False, 'shuffle': False, 'batch_size': 24, 'num_workers': 8}}, 'preprocessor': {'_target_': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor', 'dither': 0.0, 'features': '${model.n_mel_channels}', 'frame_splicing': 1, 'highfreq': None, 'log': True, 'log_zero_guard_type': 'add', 'log_zero_guard_value': 1e-05, 'lowfreq': 0, 'mag_power': 1.0, 'n_fft': '${model.n_window_size}', 'n_window_size': '${model.n_window_size}', 'n_window_stride': '${model.n_window_stride}', 'normalize': None, 'pad_to': 1, 'pad_value': 0, 'preemph': None, 'sample_rate': '${sample_rate}', 'window': 'hann', 'window_size': None, 'window_stride': None}, 'input_fft': {'_target_': 'nemo.collections.tts.modules.transformer.FFTransformerEncoder', 'n_layer': 6, 'n_head': 1, 'd_model': '${model.symbols_embedding_dim}', 'd_head': 64, 'd_inner': 1536, 'kernel_size': 3, 'dropout': 0.1, 'dropatt': 0.1, 'dropemb': 0.0, 'd_embed': '${model.symbols_embedding_dim}'}, 'output_fft': {'_target_': 'nemo.collections.tts.modules.transformer.FFTransformerDecoder', 'n_layer': 6, 'n_head': 1, 'd_model': '${model.symbols_embedding_dim}', 'd_head': 64, 'd_inner': 1536, 'kernel_size': 3, 'dropout': 0.1, 'dropatt': 0.1, 'dropemb': 0.0}, 'alignment_module': {'_target_': 'nemo.collections.tts.modules.aligner.AlignmentEncoder', 'n_text_channels': '${model.symbols_embedding_dim}'}, 'duration_predictor': {'_target_': 'nemo.collections.tts.modules.fastpitch.TemporalPredictor', 'input_size': '${model.symbols_embedding_dim}', 'kernel_size': 3, 'filter_size': 256, 'dropout': 0.1, 'n_layers': 2}, 'pitch_predictor': {'_target_': 'nemo.collections.tts.modules.fastpitch.TemporalPredictor', 'input_size': '${model.symbols_embedding_dim}', 'kernel_size': 3, 'filter_size': 256, 'dropout': 0.1, 'n_layers': 2}, 'optim': {'name': 'adam', 'lr': 0.0002, 'betas': [0.9, 0.98], 'weight_decay': 1e-06}}, 'trainer': {'gpus': -1, 'max_epochs': 10, 'num_nodes': 1, 'accelerator': 'ddp', 'accumulate_grad_batches': 1, 'checkpoint_callback': False, 'logger': False, 'gradient_clip_val': 1000.0, 'flush_logs_every_n_steps': 1000, 'log_every_n_steps': 100, 'check_val_every_n_epoch': 1}, 'exp_manager': {'exp_dir': '/root/data/VCC2020-database/extract/target_task1/exp_base/TEM1_to_TEF1_no_mixing_5_mins', 'name': '${name}', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'monitor': 'v_loss'}}, 'init_from_ptl_ckpt': '/root/data/VCC2020-database/extract/target_task1/exp_base/TEM1_to_TEF1_no_mixing_5_mins/FastPitch/2021-09-14_19-16-49/checkpoints/FastPitch--v_loss=17.86-epoch=9-last.ckpt'}\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt)#, cfg={'name': 'FastPitch', 'sample_rate': 44100, 'train_dataset': '/root/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json', 'validation_datasets': '/root/data/VCC2020-database/extract/target_task1/filelist/TEF1_metadata_dur_5_mins_local.json', 'prior_folder': '/root/data/VCC2020-database/extract/target_task1/json/PriorsTEF1', 'model': {'learn_alignment': True, 'n_speakers': 1, 'symbols_embedding_dim': 384, 'max_token_duration': 75, 'n_mel_channels': 80, 'pitch_embedding_kernel_size': 3, 'n_window_size': 2048, 'n_window_stride': 512, 'fmax': None, 'pitch_fmin': 80, 'pitch_fmax': 640, 'pitch_avg': 211.27540199742586, 'pitch_std': 52.1851002822779, 'train_ds': {'dataset': {'_target_': 'nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset', 'manifest_filepath': '${train_dataset}', 'max_duration': None, 'min_duration': 0.1, 'int_values': False, 'normalize': True, 'sample_rate': '${sample_rate}', 'trim': False, 'sup_data_path': '${prior_folder}', 'n_window_stride': '${model.n_window_stride}', 'n_window_size': '${model.n_window_size}', 'pitch_fmin': '${model.pitch_fmin}', 'pitch_fmax': '${model.pitch_fmax}', 'pitch_avg': '${model.pitch_avg}', 'pitch_std': '${model.pitch_std}', 'vocab': {'notation': 'phonemes', 'punct': True, 'spaces': True, 'stresses': True, 'add_blank_at': 'None', 'pad_with_space': True, 'chars': True, 'improved_version_g2p': True}}, 'dataloader_params': {'drop_last': False, 'shuffle': True, 'batch_size': 24, 'num_workers': 12}}, 'validation_ds': {'dataset': {'_target_': 'nemo.collections.asr.data.audio_to_text.AudioToCharWithPriorAndPitchDataset', 'manifest_filepath': '${validation_datasets}', 'max_duration': None, 'min_duration': None, 'int_values': False, 'normalize': True, 'sample_rate': '${sample_rate}', 'trim': False, 'sup_data_path': '${prior_folder}', 'n_window_stride': '${model.n_window_stride}', 'n_window_size': '${model.n_window_size}', 'pitch_fmin': '${model.pitch_fmin}', 'pitch_fmax': '${model.pitch_fmax}', 'pitch_avg': '${model.pitch_avg}', 'pitch_std': '${model.pitch_std}', 'vocab': {'notation': 'phonemes', 'punct': True, 'spaces': True, 'stresses': True, 'add_blank_at': 'None', 'pad_with_space': True, 'chars': True, 'improved_version_g2p': True}}, 'dataloader_params': {'drop_last': False, 'shuffle': False, 'batch_size': 24, 'num_workers': 8}}, 'preprocessor': {'_target_': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor', 'dither': 0.0, 'features': '${model.n_mel_channels}', 'frame_splicing': 1, 'highfreq': None, 'log': True, 'log_zero_guard_type': 'add', 'log_zero_guard_value': 1e-05, 'lowfreq': 0, 'mag_power': 1.0, 'n_fft': '${model.n_window_size}', 'n_window_size': '${model.n_window_size}', 'n_window_stride': '${model.n_window_stride}', 'normalize': None, 'pad_to': 1, 'pad_value': 0, 'preemph': None, 'sample_rate': '${sample_rate}', 'window': 'hann', 'window_size': None, 'window_stride': None}, 'input_fft': {'_target_': 'nemo.collections.tts.modules.transformer.FFTransformerEncoder', 'n_layer': 6, 'n_head': 1, 'd_model': '${model.symbols_embedding_dim}', 'd_head': 64, 'd_inner': 1536, 'kernel_size': 3, 'dropout': 0.1, 'dropatt': 0.1, 'dropemb': 0.0, 'd_embed': '${model.symbols_embedding_dim}'}, 'output_fft': {'_target_': 'nemo.collections.tts.modules.transformer.FFTransformerDecoder', 'n_layer': 6, 'n_head': 1, 'd_model': '${model.symbols_embedding_dim}', 'd_head': 64, 'd_inner': 1536, 'kernel_size': 3, 'dropout': 0.1, 'dropatt': 0.1, 'dropemb': 0.0}, 'alignment_module': {'_target_': 'nemo.collections.tts.modules.aligner.AlignmentEncoder', 'n_text_channels': '${model.symbols_embedding_dim}'}, 'duration_predictor': {'_target_': 'nemo.collections.tts.modules.fastpitch.TemporalPredictor', 'input_size': '${model.symbols_embedding_dim}', 'kernel_size': 3, 'filter_size': 256, 'dropout': 0.1, 'n_layers': 2}, 'pitch_predictor': {'_target_': 'nemo.collections.tts.modules.fastpitch.TemporalPredictor', 'input_size': '${model.symbols_embedding_dim}', 'kernel_size': 3, 'filter_size': 256, 'dropout': 0.1, 'n_layers': 2}, 'optim': {'name': 'adam', 'lr': 0.0002, 'betas': [0.9, 0.98], 'weight_decay': 1e-06}}, 'trainer': {'gpus': -1, 'max_epochs': 10, 'num_nodes': 1, 'accelerator': 'ddp', 'accumulate_grad_batches': 1, 'checkpoint_callback': False, 'logger': False, 'gradient_clip_val': 1000.0, 'flush_logs_every_n_steps': 1000, 'log_every_n_steps': 100, 'check_val_every_n_epoch': 1}, 'exp_manager': {'exp_dir': '/root/data/VCC2020-database/extract/target_task1/exp_base/TEM1_to_TEF1_no_mixing_5_mins', 'name': '${name}', 'create_tensorboard_logger': True, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'monitor': 'v_loss'}}, 'init_from_ptl_ckpt': '/root/data/VCC2020-database/extract/target_task1/exp_base/TEM1_to_TEF1_no_mixing_5_mins/FastPitch/2021-09-14_19-16-49/checkpoints/FastPitch--v_loss=17.86-epoch=9-last.ckpt'})\n",
    "spec_model.eval().cuda()\n",
    "_speaker=None\n",
    "if mixing:\n",
    "    _speaker = 1\n",
    "\n",
    "num_val = 2\n",
    "\n",
    "manifest_path = os.path.join(filelist_dir, \"{}_mainifest_dev_ns_all_local.json\".format(new_speaker_id))\n",
    "val_records = []\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append( json.loads(line) )\n",
    "        if len(val_records) >= num_val:\n",
    "            break\n",
    "            \n",
    "for val_record in val_records:\n",
    "    print (\"Real validation audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=44100))\n",
    "    print (\"SYNTHESIZED FOR -- Speaker: {} | Dataset size: {} mins | Mixing:{} | Text: {}\".format(new_speaker_id, duration_mins, mixing, val_record['text']))\n",
    "    spec, audio = infer(spec_model, vocoder, val_record['text'], speaker = _speaker)\n",
    "    ipd.display(ipd.Audio(audio, rate=44100))\n",
    "    %matplotlib inline\n",
    "    #if spec is not None:\n",
    "    imshow(spec, origin=\"lower\", aspect = \"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f20ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "TTS_FastPitch_Finetuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
